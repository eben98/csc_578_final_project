# -*- coding: utf-8 -*-
"""Final Project CSC 578.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zem0Zasu04_El33RtoLQiGhFuOjtNTM9

# Predicting Hourly Energy Consumption with Weather Data
**Project:** A Comparative Analysis of RNN, LSTM, and CNN-LSTM Models

This notebook provides a complete workflow for forecasting hourly energy consumption in the ComEd region. It includes the data preparation pipeline and the implementation, training, and evaluation of four different neural network architectures:
1.  **Simple Recurrent Neural Network (RNN)**
2.  **Long Short-Term Memory (LSTM) Network**
3.  **Baseline CNN-LSTM Hybrid Model**
4.  **Advanced CNN-LSTM Hybrid Model**

The goal is to compare these approaches and identify the most effective architecture for this time-series forecasting problem.

## 1. Introduction to Libraries

Before we begin, we need to import the necessary Python libraries. These tools provide the foundation for data manipulation, model building, and visualization.

**Installation Instructions:**
If you do not have these libraries installed, you can install them using pip:
```bash
pip install numpy pandas torch scikit-learn matplotlib seaborn tqdm
```

**Library Roles:**

* **`numpy`**: The fundamental package for numerical computing in Python. We use it to create and manage the multi-dimensional arrays (tensors) that our model processes.
* **`pandas`**: An essential tool for data analysis and manipulation. We use it to load our data from the CSV file into a structured DataFrame, making it easy to clean and engineer features.
* **`torch` (PyTorch)**: An open-source machine learning framework that is the backbone of our project. We use it to define the model's architecture, manage tensors, and run the training process. **TensorFlow is not used in this notebook.**
* **`torch.nn` and `torch.utils.data`**: Specific modules within PyTorch. `nn` provides the building blocks for our neural network (like layers, activation functions, and loss functions). `utils.data` provides helpful classes like `TensorDataset` and `DataLoader` to efficiently handle and batch our data.
* **`scikit-learn`**: A comprehensive machine learning library. While PyTorch builds our models, we use scikit-learn's powerful utilities for data preprocessing (`StandardScaler`) and evaluation (`mean_squared_error`, `mean_absolute_percentage_error`).
* **`matplotlib` & `seaborn`**: Widely-used libraries for creating static, animated, and interactive visualizations. We use them to plot our results and training progress.
* **`tqdm`**: A useful utility for creating smart progress bars for our training loops.
"""

# --- Library Imports ---
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error
import matplotlib.pyplot as plt
import seaborn as sns
import copy
from tqdm.notebook import tqdm

# --- Setup for Reproducibility ---
# Setting random seeds ensures that our results are reproducible.
torch.manual_seed(42)
np.random.seed(42)
plt.style.use('seaborn-v0_8-whitegrid')

print("Libraries imported and seeds set.")

"""## 2. Data Loading & Pre-processing

This section outlines the common data preparation pipeline used for all four models to ensure a fair comparison. The process involves loading the data, performing feature engineering, splitting the data chronologically, scaling the features, and creating sequential windows for our time-series models.
"""

# --- Step 2.1: Load Data and Engineer Features ---

# Load the dataset from the CSV file.
df = pd.read_csv('merged_energy_weather.csv')

# Rename the time column and set it as a datetime index.
df.rename(columns={'Unnamed: 0': 'datetime'}, inplace=True)
df['datetime'] = pd.to_datetime(df['datetime'], utc=True)
df.set_index('datetime', inplace=True)
df.sort_index(inplace=True)

# Fill any potential missing values using linear interpolation.
df.interpolate(method='linear', limit_direction='forward', inplace=True)

# Engineer new features from the datetime index to help the model learn patterns.
df['hour'] = df.index.hour
df['dayofweek'] = df.index.dayofweek
df['month'] = df.index.month
df['dayofyear'] = df.index.dayofyear
df['year'] = df.index.year
df['is_weekend'] = df['dayofweek'].isin([5, 6]).astype(int)

# Use all available columns as features, ensuring the target 'COMED_MW' is the first column.
all_other_columns = [col for col in df.columns if col != 'COMED_MW']
feature_columns = ['COMED_MW'] + all_other_columns
features_df = df[feature_columns].copy()

print(f"Data loaded and features engineered. Total features: {len(features_df.columns)}")

# --- Step 2.2: Chronological Data Split and Scaling ---

# Define the sizes for our training, validation, and test sets.
train_size = int(len(features_df) * 0.70)
val_size = int(len(features_df) * 0.15)
test_size = len(features_df) - train_size - val_size

# Split the data chronologically.
train_df = features_df.iloc[:train_size]
val_df = features_df.iloc[train_size:train_size + val_size]
test_df = features_df.iloc[train_size + val_size:]

print(f"Train set size: {len(train_df)}")
print(f"Validation set size: {len(val_df)}")
print(f"Test set size: {len(test_df)}")

# Using StandardScaler to normalize features to have a mean of 0 and variance of 1.
# This is a crucial step for stable neural network training.
scaler = StandardScaler()

# Fit the scaler ONLY on the training data to prevent data leakage.
train_scaled = scaler.fit_transform(train_df)
# Then, use the already-fitted scaler to transform the other two sets.
val_scaled = scaler.transform(val_df)
test_scaled = scaler.transform(test_df)

print("\nData split and scaled correctly.")

# --- Step 2.3: Sequence Creation (Windowing) ---

def create_sequences(data, n_steps):
    X, y = [], []
    for i in range(len(data) - n_steps):
        # The input is a sequence of length 'n_steps'
        seq_x = data[i:i + n_steps, :]
        # The target is the 'COMED_MW' value (first column) of the next time step
        seq_y = data[i + n_steps, 0]
        X.append(seq_x)
        y.append(seq_y)
    return np.array(X), np.array(y)

# Use a lookback window of 24 hours.
N_STEPS = 24

# Create the sequences for each of our datasets.
X_train, y_train = create_sequences(train_scaled, N_STEPS)
X_val, y_val = create_sequences(val_scaled, N_STEPS)
X_test, y_test = create_sequences(test_scaled, N_STEPS)

# Convert the numpy arrays to PyTorch Tensors.
X_train = torch.from_numpy(X_train).float()
y_train = torch.from_numpy(y_train).float().view(-1, 1)
X_val = torch.from_numpy(X_val).float()
y_val = torch.from_numpy(y_val).float().view(-1, 1)
X_test = torch.from_numpy(X_test).float()
y_test = torch.from_numpy(y_test).float().view(-1, 1)

print("Time-series sequences created.")
print(f"Training input shape: {X_train.shape}")

"""## 3. Model Design and Implementation

In this section, we define the architectures for the four different models we will be comparing. Each model is encapsulated in its own PyTorch `nn.Module` class.

### 3.1 Simple RNN Model

**Architecture:** This is the most basic recurrent model. It consists of a standard `nn.RNN` layer to process the sequence, followed by a linear layer to produce the final prediction. RNNs can struggle with long-term dependencies due to the vanishing gradient problem but serve as an important baseline.
"""

# --- Model 1: Simple RNN ---
class SimpleRNN(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers, output_size):
        super(SimpleRNN, self).__init__()
        self.rnn = nn.RNN(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True
        )
        self.fc = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        rnn_out, _ = self.rnn(x)
        last_time_step_out = rnn_out[:, -1, :]
        out = self.fc(last_time_step_out)
        return out.squeeze(-1)

"""### 3.2 LSTM Model

**Architecture:** The LSTM model improves upon the RNN by introducing gating mechanisms (input, forget, and output gates). This allows the network to better control the flow of information and remember important patterns over longer sequences, mitigating the vanishing gradient problem.
"""

# --- Model 2: LSTM ---
class EnergyLSTM(nn.Module):
    def __init__(self, n_features, hidden_size, num_layers):
        super(EnergyLSTM, self).__init__()
        self.lstm = nn.LSTM(
            input_size=n_features,
            hidden_size=hidden_size,
            num_layers=num_layers,
            batch_first=True
        )
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x):
        lstm_out, _ = self.lstm(x)
        out = lstm_out[:, -1, :]  # Take output of the last time step
        return self.fc(out).squeeze(-1)

"""### 3.3 CNN-LSTM Hybrid Models

**Architecture:** The hybrid models combine a Convolutional Neural Network (CNN) with an LSTM. The 1D CNN layer acts as a feature extractor, identifying patterns and interactions *across the 98 features* at each time step. The output of the CNN, which is a sequence of learned feature maps, is then passed to the LSTM layers to learn the temporal dynamics.

* **Baseline CNN-LSTM:** Uses a 2-layer LSTM.
* **Advanced CNN-LSTM:** Uses a deeper 3-layer LSTM for increased capacity.
"""

# --- Model 3: Baseline CNN-LSTM (2-layer) ---
class BaselineCNNLSTM(nn.Module):
    def __init__(self, input_size, cnn_out_channels, lstm_hidden_size, lstm_num_layers, output_size):
        super(BaselineCNNLSTM, self).__init__()
        self.cnn = nn.Sequential(
            nn.Conv1d(in_channels=input_size, out_channels=cnn_out_channels, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.Conv1d(in_channels=cnn_out_channels, out_channels=cnn_out_channels // 2, kernel_size=3, padding=1),
            nn.ReLU()
        )
        self.lstm = nn.LSTM(
            input_size=cnn_out_channels // 2,
            hidden_size=lstm_hidden_size,
            num_layers=lstm_num_layers,
            batch_first=True
        )
        self.fc = nn.Linear(lstm_hidden_size, output_size)

    def forward(self, x):
        x = x.permute(0, 2, 1)
        x = self.cnn(x)
        x = x.permute(0, 2, 1)
        lstm_out, _ = self.lstm(x)
        last_time_step_out = lstm_out[:, -1, :]
        out = self.fc(last_time_step_out)
        return out.squeeze(-1)


# --- Model 4: Advanced CNN-LSTM (3-layer) ---
class AdvancedCNNLSTM(nn.Module):
    def __init__(self, n_features, cnn_filters=128, lstm_units=128, dropout=0.3):
        super(AdvancedCNNLSTM, self).__init__()
        self.conv1d = nn.Conv1d(
            in_channels=n_features,
            out_channels=cnn_filters,
            kernel_size=2,
            padding='same'
        )
        self.relu = nn.ReLU()
        self.lstm = nn.LSTM(
            input_size=cnn_filters,
            hidden_size=lstm_units,
            num_layers=3,
            batch_first=True,
            dropout=dropout
        )
        self.fc = nn.Linear(in_features=lstm_units, out_features=1)

    def forward(self, x):
        x = x.permute(0, 2, 1)
        x = self.conv1d(x)
        x = self.relu(x)
        x = x.permute(0, 2, 1)
        _, (h_n, _) = self.lstm(x)
        last_hidden_state = h_n[-1]
        out = self.fc(last_hidden_state)
        return out.squeeze(-1)

"""## 4. Training Process

This section outlines the training pipeline. We define a generic `train_model` function that can be used to train any of the model architectures we've defined. This function incorporates a training loop, a validation loop, and an early stopping mechanism to prevent overfitting.
"""

# --- Generic Training Function ---
def train_model(model, model_name, train_loader, val_loader, criterion, optimizer, epochs, device, patience=5):
    print(f'--- Starting Training for {model_name} ---')
    history = {'train_loss': [], 'val_loss': []}
    best_val_loss = float('inf')
    early_stopping_counter = 0

    for epoch in range(epochs):
        model.train()
        epoch_train_loss = 0.0

        train_loop = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs} [Train]", leave=False)
        for inputs, labels in train_loop:
            inputs, labels = inputs.to(device), labels.view(-1).to(device)

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)
            optimizer.step()
            epoch_train_loss += loss.item()

        model.eval()
        epoch_val_loss = 0.0
        val_loop = tqdm(val_loader, desc=f"Epoch {epoch+1}/{epochs} [Val]", leave=False)
        with torch.no_grad():
            for inputs, labels in val_loop:
                inputs, labels = inputs.to(device), labels.view(-1).to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                epoch_val_loss += loss.item()

        avg_train_loss = epoch_train_loss / len(train_loader)
        avg_val_loss = epoch_val_loss / len(val_loader)
        history['train_loss'].append(avg_train_loss)
        history['val_loss'].append(avg_val_loss)

        print(f"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}")

        # # Early stopping logic
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), f'{model_name}_best.pth')
            print(f"   -> Validation loss decreased. Saving model as {model_name}_best.pth")
            early_stopping_counter = 0
        else:
            early_stopping_counter += 1
            print(f"   -> Val loss did not improve. Counter: {early_stopping_counter}/{patience}")

        if early_stopping_counter >= patience:
            print(f"--- Early stopping triggered for {model_name}. ---")
            break

    print(f"--- Finished Training for {model_name} ---")
    model.load_state_dict(torch.load(f'{model_name}_best.pth'))
    return model, history

"""### 4.1 Training Each Model

Now we will instantiate and train each of our four models using the generic training function. The progress and loss for each epoch will be printed.
"""

# --- Setup for All Models ---
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
N_FEATURES = X_train.shape[2]
LEARNING_RATE = 0.001
EPOCHS = 20
BATCH_SIZE = 64

# --- Create DataLoaders ---
train_dataset = TensorDataset(X_train, y_train)
val_dataset = TensorDataset(X_val, y_val)
test_dataset = TensorDataset(X_test, y_test)

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

# --- Store trained models and histories ---
trained_models = {}
histories = {}

# --- Train Simple RNN ---
rnn_model = SimpleRNN(input_size=N_FEATURES, hidden_size=128, num_layers=2, output_size=1).to(device)
rnn_optimizer = torch.optim.Adam(rnn_model.parameters(), lr=LEARNING_RATE)
rnn_criterion = nn.MSELoss()
trained_models['RNN'], histories['RNN'] = train_model(
    rnn_model, 'RNN', train_loader, val_loader, rnn_criterion, rnn_optimizer, EPOCHS, device
)

# --- Train LSTM ---
lstm_model = EnergyLSTM(n_features=N_FEATURES, hidden_size=64, num_layers=2).to(device)
lstm_optimizer = torch.optim.Adam(lstm_model.parameters(), lr=LEARNING_RATE)
lstm_criterion = nn.MSELoss()
trained_models['LSTM'], histories['LSTM'] = train_model(
    lstm_model, 'LSTM', train_loader, val_loader, lstm_criterion, lstm_optimizer, EPOCHS, device
)

# --- Train Baseline CNN-LSTM ---
baseline_cnn_lstm_model = BaselineCNNLSTM(
    input_size=N_FEATURES, cnn_out_channels=64, lstm_hidden_size=128, lstm_num_layers=2, output_size=1
).to(device)
baseline_optimizer = torch.optim.Adam(baseline_cnn_lstm_model.parameters(), lr=LEARNING_RATE)
baseline_criterion = nn.MSELoss()
trained_models['Baseline_CNN_LSTM'], histories['Baseline_CNN_LSTM'] = train_model(
    baseline_cnn_lstm_model, 'Baseline_CNN_LSTM', train_loader, val_loader, baseline_criterion, baseline_optimizer, EPOCHS, device
)

# --- Train Advanced CNN-LSTM ---
advanced_cnn_lstm_model = AdvancedCNNLSTM(n_features=N_FEATURES).to(device)
advanced_optimizer = torch.optim.Adam(advanced_cnn_lstm_model.parameters(), lr=LEARNING_RATE)
advanced_criterion = nn.MSELoss()
trained_models['Advanced_CNN_LSTM'], histories['Advanced_CNN_LSTM'] = train_model(
    advanced_cnn_lstm_model, 'Advanced_CNN_LSTM', train_loader, val_loader, advanced_criterion, advanced_optimizer, EPOCHS, device, patience=5
)

"""## 5. Evaluation Results

In this final section, we evaluate and compare the performance of all four trained models on the unseen test data. We present quantitative metrics in a table and visualize the predictions of the best model.
"""

# --- Function to get predictions and evaluate ---
def evaluate_model(model, loader, device, scaler):
    model.eval()
    predictions, actuals = [], []
    with torch.no_grad():
        for X_batch, y_batch in loader:
            X_batch = X_batch.to(device)
            y_pred = model(X_batch)
            predictions.extend(y_pred.cpu().numpy())
            actuals.extend(y_batch.cpu().numpy())

    predictions = np.array(predictions).reshape(-1, 1)
    actuals = np.array(actuals).reshape(-1, 1)

    # We need a dummy array to inverse transform the target variable correctly
    # since the scaler was fit on all features.
    dummy_preds = np.zeros((len(predictions), len(feature_columns)))
    dummy_preds[:, 0] = predictions.flatten()
    predictions_mw = scaler.inverse_transform(dummy_preds)[:, 0]

    dummy_actual = np.zeros((len(actuals), len(feature_columns)))
    dummy_actual[:, 0] = actuals.flatten()
    actuals_mw = scaler.inverse_transform(dummy_actual)[:, 0]

    # Calculate metrics
    rmse = np.sqrt(mean_squared_error(actuals_mw, predictions_mw))
    mape = mean_absolute_percentage_error(actuals_mw, predictions_mw) * 100

    return rmse, mape, actuals_mw, predictions_mw

# --- Evaluate all models ---
results = {}
for name, model in trained_models.items():
    rmse, mape, _, _ = evaluate_model(model, test_loader, device, scaler)
    results[name] = {'RMSE (MW)': rmse, 'MAPE (%)': mape}

# --- Display Results in a DataFrame ---
results_df = pd.DataFrame(results).T.sort_values(by='MAPE (%)')
print("--- Comparative Performance of Forecasting Models ---")
results_df

"""### 5.1 Visualization of Best Model

We visualize the predictions from our best-performing model (Advanced CNN-LSTM) against the actual values to qualitatively assess its performance.
"""

# --- Get predictions from the best model for plotting ---
best_model_name = 'Advanced_CNN_LSTM'
best_model = trained_models[best_model_name]
_, _, actuals_mw, predictions_mw = evaluate_model(best_model, test_loader, device, scaler)

# --- Plotting ---
plt.figure(figsize=(15, 7))
plt.plot(actuals_mw[:500], label='Actual Energy Consumption (MW)', color='blue')
plt.plot(predictions_mw[:500], label='Predicted Energy Consumption (MW)', color='red', linestyle='--')
plt.title(f'Forecast vs. Actuals on Test Set (Best Model: {best_model_name})')
plt.xlabel('Time (Hours)')
plt.ylabel('Energy Consumption (MW)')
plt.legend()
plt.grid(True)
plt.show()

# Create aggregate weather features from the original DataFrame
temp_cols = [col for col in df.columns if 'temp' in col]
prcp_cols = [col for col in df.columns if 'prcp' in col]
df['avg_temp'] = df[temp_cols].mean(axis=1)
df['total_prcp'] = df[prcp_cols].sum(axis=1)

hot_threshold = df['avg_temp'].quantile(0.90)  # Top 10% hottest hours
cold_threshold = df['avg_temp'].quantile(0.10) # Bottom 10% coldest hours
mild_low = df['avg_temp'].quantile(0.40)       # 40th percentile
mild_high = df['avg_temp'].quantile(0.60)      # 60th percentile

# Filter data for each condition
hot_sunny_days = df[(df['avg_temp'] >= hot_threshold) & (df['total_prcp'] == 0)]
cold_days = df[df['avg_temp'] <= cold_threshold]
rainy_days = df[df['total_prcp'] > 0]
mild_clear_days = df[(df['avg_temp'] > mild_low) & (df['avg_temp'] < mild_high) & (df['total_prcp'] == 0)]

# Calculate the average consumption profile for each condition
hot_profile = hot_sunny_days.groupby('hour')['COMED_MW'].mean()
cold_profile = cold_days.groupby('hour')['COMED_MW'].mean()
rainy_profile = rainy_days.groupby('hour')['COMED_MW'].mean()
mild_profile = mild_clear_days.groupby('hour')['COMED_MW'].mean()

# --- Plotting the Comparison ---
plt.style.use('seaborn-v0_8-whitegrid')
plt.figure(figsize=(14, 8))

# Plot each profile with a distinct style for clarity
hot_profile.plot(label=f'Hot & Sunny Days (Temp > {hot_threshold:.1f}°C)', marker='o', linestyle='-')
cold_profile.plot(label=f'Cold Days (Temp < {cold_threshold:.1f}°C)', marker='s', linestyle='-')
rainy_profile.plot(label='Rainy Days (Any Precipitation)', marker='^', linestyle='--')
mild_profile.plot(label='Mild & Clear Days', marker='d', linestyle=':')

# Add labels, title, legend, and grid for better readability
plt.title('Average Daily Energy Consumption Profile by Weather Condition', fontsize=16)
plt.xlabel('Hour of the Day', fontsize=12)
plt.ylabel('Average Energy Consumption (MW)', fontsize=12)
plt.xticks(np.arange(0, 24, 2))
plt.legend(fontsize=11)
plt.grid(True, which='both', linestyle='--', linewidth=0.5)
plt.show()

"""### 5.2 Insights and Conclusion

**Performance Analysis:** The results clearly indicate that the **Advanced Hybrid CNN-LSTM model significantly outperforms** all other architectures, achieving the lowest RMSE and a remarkably low MAPE of 1.40%. The performance progression across the models provides valuable insights:
* The **Simple RNN** provided the weakest forecast, likely struggling with long-range dependencies.
* The **Standard LSTM** showed a dramatic improvement over the RNN, demonstrating its ability to remember patterns over time.
* The **Baseline CNN-LSTM** was less effective than the standard LSTM, suggesting that a simple hybrid architecture without sufficient depth is not optimal.
* The **Advanced CNN-LSTM** achieved the best performance by leveraging a deeper hierarchical feature extraction process. The initial CNN layer identifies complex patterns among the 98 features, creating a richer sequence for the deep 3-layer LSTM network to process, resulting in a more nuanced and accurate prediction.

**Conclusion:** This project successfully demonstrated that a well-designed deep learning model, trained on a rich, region-specific dataset, can provide highly accurate energy consumption forecasts, paving the way for a more efficient and resilient energy grid. The hybrid CNN-LSTM architecture proved to be the most effective approach for this complex time-series problem.
"""